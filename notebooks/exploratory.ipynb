{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Speech Emotion Recognition - Exploratory Data Analysis\n",
        "\n",
        "This notebook explores the CREMA-D dataset and analyzes audio features for emotion recognition.\n",
        "\n",
        "## Table of Contents\n",
        "1. Dataset Overview\n",
        "2. Audio Waveform Analysis\n",
        "3. Spectrogram Visualization\n",
        "4. Feature Extraction\n",
        "5. Emotion Distribution\n",
        "6. Statistical Analysis\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import sys\n",
        "from pathlib import Path\n",
        "sys.path.append(str(Path.cwd().parent / 'src'))\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import librosa\n",
        "import librosa.display\n",
        "import glob\n",
        "from IPython.display import Audio\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# Set style\n",
        "sns.set_style('whitegrid')\n",
        "plt.rcParams['figure.figsize'] = (12, 6)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1. Dataset Overview\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from dataset import CremaDDataset\n",
        "from config import EMOTION_NAMES, AUDIO_WAV_DIR\n",
        "\n",
        "# Create dataset instance\n",
        "dataset = CremaDDataset()\n",
        "\n",
        "# Create and load metadata\n",
        "df = dataset.create_metadata_csv()\n",
        "\n",
        "print(f\"Total samples: {len(df)}\")\n",
        "print(f\"\\nDataset shape: {df.shape}\")\n",
        "print(f\"\\nColumns: {df.columns.tolist()}\")\n",
        "print(f\"\\nFirst few rows:\")\n",
        "df.head()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Emotion distribution\n",
        "emotion_dist = dataset.get_emotion_distribution()\n",
        "print(\"\\nEmotion Distribution:\")\n",
        "print(emotion_dist)\n",
        "\n",
        "# Visualize\n",
        "plt.figure(figsize=(12, 6))\n",
        "plt.bar(emotion_dist['emotion'], emotion_dist['count'], color='skyblue', edgecolor='navy')\n",
        "plt.xlabel('Emotion', fontsize=12)\n",
        "plt.ylabel('Count', fontsize=12)\n",
        "plt.title('CREMA-D Dataset - Emotion Distribution', fontsize=14, fontweight='bold')\n",
        "for i, (emotion, count) in enumerate(zip(emotion_dist['emotion'], emotion_dist['count'])):\n",
        "    plt.text(i, count + 20, str(count), ha='center', va='bottom', fontsize=11)\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2. Audio Waveform Analysis\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Sample one file per emotion\n",
        "sample_files = {}\n",
        "for emotion in EMOTION_NAMES:\n",
        "    samples = df[df['emotion'] == emotion].head(1)\n",
        "    if len(samples) > 0:\n",
        "        sample_files[emotion] = samples['file_path'].values[0]\n",
        "\n",
        "# Plot waveforms\n",
        "fig, axes = plt.subplots(3, 2, figsize=(15, 12))\n",
        "axes = axes.flatten()\n",
        "\n",
        "for i, (emotion, file_path) in enumerate(sample_files.items()):\n",
        "    # Load audio\n",
        "    y, sr = librosa.load(file_path, sr=22050)\n",
        "    \n",
        "    # Plot\n",
        "    axes[i].plot(np.linspace(0, len(y)/sr, len(y)), y, color='blue', alpha=0.7)\n",
        "    axes[i].set_title(f'{emotion} - {Path(file_path).name}', fontsize=12, fontweight='bold')\n",
        "    axes[i].set_xlabel('Time (s)')\n",
        "    axes[i].set_ylabel('Amplitude')\n",
        "    axes[i].grid(True, alpha=0.3)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3. Mel Spectrogram Visualization\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Plot Mel spectrograms\n",
        "fig, axes = plt.subplots(3, 2, figsize=(15, 14))\n",
        "axes = axes.flatten()\n",
        "\n",
        "for i, (emotion, file_path) in enumerate(sample_files.items()):\n",
        "    # Load audio\n",
        "    y, sr = librosa.load(file_path, sr=22050)\n",
        "    \n",
        "    # Compute Mel spectrogram\n",
        "    mel_spec = librosa.feature.melspectrogram(y=y, sr=sr, n_mels=128, n_fft=2048, hop_length=512)\n",
        "    mel_spec_db = librosa.power_to_db(mel_spec, ref=np.max)\n",
        "    \n",
        "    # Plot\n",
        "    img = librosa.display.specshow(mel_spec_db, sr=sr, hop_length=512, x_axis='time', y_axis='mel', \n",
        "                                    ax=axes[i], cmap='viridis')\n",
        "    axes[i].set_title(f'{emotion} - Mel Spectrogram', fontsize=12, fontweight='bold')\n",
        "    fig.colorbar(img, ax=axes[i], format='%+2.0f dB')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4. Audio Feature Statistics\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Analyze audio statistics for each emotion\n",
        "stats = []\n",
        "\n",
        "for emotion in EMOTION_NAMES:\n",
        "    emotion_files = df[df['emotion'] == emotion]['file_path'].tolist()\n",
        "    \n",
        "    durations = []\n",
        "    energies = []\n",
        "    zero_crossing_rates = []\n",
        "    \n",
        "    # Sample 50 files per emotion\n",
        "    for file_path in emotion_files[:50]:\n",
        "        try:\n",
        "            y, sr = librosa.load(file_path, sr=22050)\n",
        "            \n",
        "            # Duration\n",
        "            duration = len(y) / sr\n",
        "            durations.append(duration)\n",
        "            \n",
        "            # Energy\n",
        "            energy = np.sum(y**2) / len(y)\n",
        "            energies.append(energy)\n",
        "            \n",
        "            # Zero crossing rate\n",
        "            zcr = np.mean(librosa.feature.zero_crossing_rate(y))\n",
        "            zero_crossing_rates.append(zcr)\n",
        "            \n",
        "        except:\n",
        "            pass\n",
        "    \n",
        "    stats.append({\n",
        "        'emotion': emotion,\n",
        "        'avg_duration': np.mean(durations),\n",
        "        'avg_energy': np.mean(energies),\n",
        "        'avg_zcr': np.mean(zero_crossing_rates),\n",
        "        'samples': len(durations)\n",
        "    })\n",
        "\n",
        "stats_df = pd.DataFrame(stats)\n",
        "print(\"Audio Feature Statistics by Emotion:\")\n",
        "print(stats_df)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Visualize statistics\n",
        "fig, axes = plt.subplots(1, 3, figsize=(18, 5))\n",
        "\n",
        "# Duration\n",
        "axes[0].bar(stats_df['emotion'], stats_df['avg_duration'], color='lightcoral', edgecolor='darkred')\n",
        "axes[0].set_title('Average Audio Duration by Emotion', fontweight='bold')\n",
        "axes[0].set_xlabel('Emotion')\n",
        "axes[0].set_ylabel('Duration (seconds)')\n",
        "\n",
        "# Energy\n",
        "axes[1].bar(stats_df['emotion'], stats_df['avg_energy'], color='lightgreen', edgecolor='darkgreen')\n",
        "axes[1].set_title('Average Energy by Emotion', fontweight='bold')\n",
        "axes[1].set_xlabel('Emotion')\n",
        "axes[1].set_ylabel('Energy')\n",
        "\n",
        "# Zero Crossing Rate\n",
        "axes[2].bar(stats_df['emotion'], stats_df['avg_zcr'], color='lightblue', edgecolor='darkblue')\n",
        "axes[2].set_title('Average Zero Crossing Rate by Emotion', fontweight='bold')\n",
        "axes[2].set_xlabel('Emotion')\n",
        "axes[2].set_ylabel('ZCR')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 5. Interactive Audio Playback\n",
        "\n",
        "Listen to sample audio files for each emotion:\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "for emotion, file_path in sample_files.items():\n",
        "    print(f\"\\n{'='*60}\")\n",
        "    print(f\"Emotion: {emotion}\")\n",
        "    print(f\"File: {Path(file_path).name}\")\n",
        "    print(f\"{'='*60}\")\n",
        "    display(Audio(file_path))\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 6. Conclusion\n",
        "\n",
        "This exploratory analysis reveals:\n",
        "\n",
        "1. **Dataset Balance**: The dataset is relatively balanced across emotions, with NEU having slightly fewer samples\n",
        "2. **Audio Characteristics**: Different emotions show distinct patterns in energy, duration, and spectral features\n",
        "3. **Spectrograms**: Mel spectrograms clearly show different patterns for different emotions\n",
        "4. **Feature Variability**: Energy and zero crossing rates vary significantly across emotions\n",
        "\n",
        "These insights will guide the model development and feature engineering process.\n",
        "\n"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
