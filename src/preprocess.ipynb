{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Audio Preprocessing for CREMA-D Dataset\n",
        "\n",
        "This notebook converts WAV files to 128-band Mel spectrograms (log scale) for speech emotion recognition.\n",
        "\n",
        "## Overview\n",
        "- Extract emotion labels from CREMA-D filenames\n",
        "- Load audio files using librosa\n",
        "- Convert to 128-band Mel spectrograms\n",
        "- Pad or crop spectrograms to fixed shape (128Ã—128)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {},
      "outputs": [],
      "source": [
        "import os\n",
        "import numpy as np\n",
        "import librosa\n",
        "from typing import Tuple, List\n",
        "import glob\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Constants and Configuration\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Emotion mapping from CREMA-D filenames\n",
        "EMOTION_MAP = {\n",
        "    'ANG': 0,  # Anger\n",
        "    'HAP': 1,  # Happiness\n",
        "    'SAD': 2,  # Sadness\n",
        "    'NEU': 3,  # Neutral\n",
        "    'DIS': 4,  # Disgust\n",
        "    'FEA': 5   # Fear\n",
        "}\n",
        "\n",
        "# Fixed spectrogram shape (time_steps, mel_bands)\n",
        "SPECTROGRAM_SHAPE = (128, 128)  # (time, frequency)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Helper Functions\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {},
      "outputs": [],
      "source": [
        "def extract_emotion_from_filename(filename: str) -> str:\n",
        "    \"\"\"\n",
        "    Extract emotion label from CREMA-D filename.\n",
        "    Format: {actor_id}_{sentence}_{emotion}_{intensity}.wav\n",
        "    Example: 1001_DFA_ANG_XX.wav -> ANG\n",
        "    \"\"\"\n",
        "    parts = os.path.basename(filename).split('_')\n",
        "    if len(parts) >= 3:\n",
        "        emotion = parts[2]\n",
        "        return emotion if emotion in EMOTION_MAP else None\n",
        "    return None\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {},
      "outputs": [],
      "source": [
        "def load_audio(file_path: str, sr: int = 22050) -> np.ndarray:\n",
        "    \"\"\"\n",
        "    Load audio file using librosa.\n",
        "    \n",
        "    Args:\n",
        "        file_path: Path to WAV file\n",
        "        sr: Sample rate (default 22050)\n",
        "    \n",
        "    Returns:\n",
        "        Audio signal as numpy array\n",
        "    \"\"\"\n",
        "    audio, _ = librosa.load(file_path, sr=sr, mono=True)\n",
        "    return audio\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {},
      "outputs": [],
      "source": [
        "def audio_to_mel_spectrogram(audio: np.ndarray, \n",
        "                             sr: int = 22050,\n",
        "                             n_mels: int = 128,\n",
        "                             n_fft: int = 2048,\n",
        "                             hop_length: int = 512) -> np.ndarray:\n",
        "    \"\"\"\n",
        "    Convert audio to 128-band Mel spectrogram (log scale).\n",
        "    \n",
        "    Args:\n",
        "        audio: Audio signal\n",
        "        sr: Sample rate\n",
        "        n_mels: Number of Mel bands (128)\n",
        "        n_fft: FFT window size\n",
        "        hop_length: Hop length for STFT\n",
        "    \n",
        "    Returns:\n",
        "        Mel spectrogram as numpy array (mel_bands, time_steps)\n",
        "    \"\"\"\n",
        "    # Compute Mel spectrogram\n",
        "    mel_spec = librosa.feature.melspectrogram(\n",
        "        y=audio,\n",
        "        sr=sr,\n",
        "        n_mels=n_mels,\n",
        "        n_fft=n_fft,\n",
        "        hop_length=hop_length\n",
        "    )\n",
        "    \n",
        "    # Convert to log scale (dB)\n",
        "    mel_spec_db = librosa.power_to_db(mel_spec, ref=np.max)\n",
        "    \n",
        "    return mel_spec_db\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {},
      "outputs": [],
      "source": [
        "def pad_or_crop_spectrogram(spectrogram: np.ndarray, \n",
        "                            target_shape: Tuple[int, int] = SPECTROGRAM_SHAPE) -> np.ndarray:\n",
        "    \"\"\"\n",
        "    Pad or crop spectrogram to fixed shape.\n",
        "    \n",
        "    Args:\n",
        "        spectrogram: Input spectrogram (mel_bands, time_steps)\n",
        "        target_shape: Target shape (time_steps, mel_bands)\n",
        "    \n",
        "    Returns:\n",
        "        Padded/cropped spectrogram with shape (time_steps, mel_bands)\n",
        "    \"\"\"\n",
        "    mel_bands, time_steps = spectrogram.shape\n",
        "    target_time, target_mel = target_shape\n",
        "    \n",
        "    # Transpose to (time_steps, mel_bands) for easier processing\n",
        "    spec = spectrogram.T  # (time_steps, mel_bands)\n",
        "    \n",
        "    # Crop or pad time dimension\n",
        "    if time_steps > target_time:\n",
        "        # Crop: take middle portion\n",
        "        start = (time_steps - target_time) // 2\n",
        "        spec = spec[start:start + target_time, :]\n",
        "    elif time_steps < target_time:\n",
        "        # Pad: add zeros at the end\n",
        "        pad_width = target_time - time_steps\n",
        "        spec = np.pad(spec, ((0, pad_width), (0, 0)), mode='constant', constant_values=0)\n",
        "    \n",
        "    # Crop or pad mel dimension\n",
        "    if mel_bands > target_mel:\n",
        "        # Crop: take first target_mel bands\n",
        "        spec = spec[:, :target_mel]\n",
        "    elif mel_bands < target_mel:\n",
        "        # Pad: add zeros\n",
        "        pad_width = target_mel - mel_bands\n",
        "        spec = np.pad(spec, ((0, 0), (0, pad_width)), mode='constant', constant_values=0)\n",
        "    \n",
        "    return spec\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Main Processing Functions\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {},
      "outputs": [],
      "source": [
        "def process_audio_file(file_path: str) -> Tuple[np.ndarray, int]:\n",
        "    \"\"\"\n",
        "    Process a single audio file: load, convert to Mel spectrogram, pad/crop.\n",
        "    \n",
        "    Args:\n",
        "        file_path: Path to WAV file\n",
        "    \n",
        "    Returns:\n",
        "        Tuple of (spectrogram, emotion_label)\n",
        "        Returns (None, None) if emotion cannot be extracted\n",
        "    \"\"\"\n",
        "    # Extract emotion label\n",
        "    emotion = extract_emotion_from_filename(file_path)\n",
        "    if emotion is None or emotion not in EMOTION_MAP:\n",
        "        return None, None\n",
        "    \n",
        "    # Load audio\n",
        "    audio = load_audio(file_path)\n",
        "    \n",
        "    # Convert to Mel spectrogram\n",
        "    mel_spec = audio_to_mel_spectrogram(audio)\n",
        "    \n",
        "    # Pad or crop to fixed shape\n",
        "    mel_spec_fixed = pad_or_crop_spectrogram(mel_spec)\n",
        "    \n",
        "    # Get emotion label index\n",
        "    label = EMOTION_MAP[emotion]\n",
        "    \n",
        "    return mel_spec_fixed, label\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {},
      "outputs": [],
      "source": [
        "def load_dataset(data_dir: str) -> Tuple[np.ndarray, np.ndarray]:\n",
        "    \"\"\"\n",
        "    Load all WAV files from directory and convert to spectrograms.\n",
        "    \n",
        "    Args:\n",
        "        data_dir: Path to AudioWAV directory\n",
        "    \n",
        "    Returns:\n",
        "        Tuple of (spectrograms, labels)\n",
        "        spectrograms: numpy array of shape (n_samples, time_steps, mel_bands, 1)\n",
        "        labels: numpy array of shape (n_samples,)\n",
        "    \"\"\"\n",
        "    # Find all WAV files\n",
        "    wav_files = glob.glob(os.path.join(data_dir, '*.wav'))\n",
        "    \n",
        "    spectrograms = []\n",
        "    labels = []\n",
        "    \n",
        "    print(f\"Processing {len(wav_files)} audio files...\")\n",
        "    \n",
        "    for i, file_path in enumerate(wav_files):\n",
        "        if (i + 1) % 500 == 0:\n",
        "            print(f\"Processed {i + 1}/{len(wav_files)} files...\")\n",
        "        \n",
        "        spec, label = process_audio_file(file_path)\n",
        "        \n",
        "        if spec is not None and label is not None:\n",
        "            # Add channel dimension for CNN: (time, mel) -> (time, mel, 1)\n",
        "            spec = np.expand_dims(spec, axis=-1)\n",
        "            spectrograms.append(spec)\n",
        "            labels.append(label)\n",
        "    \n",
        "    print(f\"Successfully processed {len(spectrograms)} files.\")\n",
        "    \n",
        "    # Convert to numpy arrays\n",
        "    X = np.array(spectrograms)\n",
        "    y = np.array(labels)\n",
        "    \n",
        "    return X, y\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {},
      "outputs": [],
      "source": [
        "def get_emotion_names() -> List[str]:\n",
        "    \"\"\"Get list of emotion names in order.\"\"\"\n",
        "    return ['ANG', 'HAP', 'SAD', 'NEU', 'DIS', 'FEA']\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Test Preprocessing Functions\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Found audio directory: /Users/macbook/Library/CloudStorage/OneDrive-SwinburneUniversity/Documents/Project/voice/CREMA-D/AudioWAV\n",
            "Directory exists: True\n"
          ]
        }
      ],
      "source": [
        "# Check if AudioWAV directory exists\n",
        "\n",
        "# Try multiple possible paths\n",
        "possible_paths = [\n",
        "    os.path.join(os.path.dirname(os.getcwd()), 'CREMA-D', 'AudioWAV'),  # From src directory\n",
        "    os.path.join(os.getcwd(), 'CREMA-D', 'AudioWAV'),  # From voice directory\n",
        "    '../CREMA-D/AudioWAV',  # Relative path\n",
        "    'CREMA-D/AudioWAV',  # Current directory\n",
        "]\n",
        "\n",
        "audio_dir = None\n",
        "for path in possible_paths:\n",
        "    if os.path.exists(path):\n",
        "        audio_dir = path\n",
        "        break\n",
        "\n",
        "if audio_dir is None:\n",
        "    print(\"Error: Could not find CREMA-D/AudioWAV directory\")\n",
        "    print(\"Tried paths:\")\n",
        "    for path in possible_paths:\n",
        "        print(f\"  - {path}\")\n",
        "else:\n",
        "    print(f\"Found audio directory: {audio_dir}\")\n",
        "    print(f\"Directory exists: {os.path.exists(audio_dir)}\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Found 7442 WAV files\n",
            "Testing with first 5 files...\n",
            "\n",
            "File 1: 1022_ITS_ANG_XX.wav\n",
            "  Extracted emotion: ANG\n",
            "  Spectrogram shape: (128, 128)\n",
            "  Label: 0 (ANG)\n",
            "\n",
            "File 2: 1037_ITS_ANG_XX.wav\n",
            "  Extracted emotion: ANG\n",
            "  Spectrogram shape: (128, 128)\n",
            "  Label: 0 (ANG)\n",
            "\n",
            "File 3: 1060_ITS_NEU_XX.wav\n",
            "  Extracted emotion: NEU\n",
            "  Spectrogram shape: (128, 128)\n",
            "  Label: 3 (NEU)\n",
            "\n",
            "File 4: 1075_ITS_NEU_XX.wav\n",
            "  Extracted emotion: NEU\n",
            "  Spectrogram shape: (128, 128)\n",
            "  Label: 3 (NEU)\n",
            "\n",
            "File 5: 1073_IOM_DIS_XX.wav\n",
            "  Extracted emotion: DIS\n",
            "  Spectrogram shape: (128, 128)\n",
            "  Label: 4 (DIS)\n",
            "\n",
            "Preprocessing functions test completed successfully!\n"
          ]
        }
      ],
      "source": [
        "# Test with a few sample files\n",
        "if audio_dir is None:\n",
        "    print(\"Cannot test: AudioWAV directory not found\")\n",
        "else:\n",
        "    wav_files = glob.glob(os.path.join(audio_dir, '*.wav'))\n",
        "    \n",
        "    if len(wav_files) == 0:\n",
        "        print(f\"Error: No WAV files found in {audio_dir}\")\n",
        "    else:\n",
        "        print(f\"\\nFound {len(wav_files)} WAV files\")\n",
        "        print(f\"Testing with first 5 files...\\n\")\n",
        "        \n",
        "        # Test processing a few files\n",
        "        for i, file_path in enumerate(wav_files[:5]):\n",
        "            filename = os.path.basename(file_path)\n",
        "            emotion = extract_emotion_from_filename(file_path)\n",
        "            print(f\"File {i+1}: {filename}\")\n",
        "            print(f\"  Extracted emotion: {emotion}\")\n",
        "            \n",
        "            if emotion:\n",
        "                spec, label = process_audio_file(file_path)\n",
        "                if spec is not None:\n",
        "                    print(f\"  Spectrogram shape: {spec.shape}\")\n",
        "                    print(f\"  Label: {label} ({get_emotion_names()[label]})\")\n",
        "                else:\n",
        "                    print(f\"  Failed to process file\")\n",
        "            else:\n",
        "                print(f\"  Could not extract emotion\")\n",
        "            print()\n",
        "        \n",
        "        print(\"Preprocessing functions test completed successfully!\")\n",
        "\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "voice-recognition",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.25"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
