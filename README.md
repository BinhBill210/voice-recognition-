# üé§ Speech Emotion Recognition

D·ª± √°n nh·∫≠n di·ªán c·∫£m x√∫c t·ª´ gi·ªçng n√≥i s·ª≠ d·ª•ng Deep Learning v·ªõi dataset CREMA-D.

[![Python](https://img.shields.io/badge/Python-3.9+-blue.svg)](https://www.python.org/)
[![TensorFlow](https://img.shields.io/badge/TensorFlow-2.15+-orange.svg)](https://www.tensorflow.org/)
[![License](https://img.shields.io/badge/License-MIT-green.svg)](LICENSE)

---

## üìã M·ª•c l·ª•c

- [Gi·ªõi thi·ªáu](#-gi·ªõi-thi·ªáu)
- [Dataset](#-dataset)
- [Ki·∫øn tr√∫c Model](#-ki·∫øn-tr√∫c-model)
- [C√†i ƒë·∫∑t](#-c√†i-ƒë·∫∑t)
- [S·ª≠ d·ª•ng](#-s·ª≠-d·ª•ng)
- [C·∫•u tr√∫c Project](#-c·∫•u-tr√∫c-project)
- [API Documentation](#-api-documentation)
- [K·∫øt qu·∫£](#-k·∫øt-qu·∫£)
- [Demo](#-demo)
- [Roadmap](#-roadmap)

---

## üéØ Gi·ªõi thi·ªáu

Project n√†y x√¢y d·ª±ng m·ªôt h·ªá th·ªëng **nh·∫≠n di·ªán c·∫£m x√∫c t·ª´ gi·ªçng n√≥i** s·ª≠ d·ª•ng Convolutional Neural Network (CNN) v·ªõi Mel Spectrogram. H·ªá th·ªëng c√≥ th·ªÉ:

- ‚úÖ Nh·∫≠n di·ªán 6 c·∫£m x√∫c: Anger, Happiness, Sadness, Neutral, Disgust, Fear
- ‚úÖ X·ª≠ l√Ω audio files (WAV format)
- ‚úÖ Ghi √¢m real-time t·ª´ microphone
- ‚úÖ Web interface v·ªõi Streamlit
- ‚úÖ Batch prediction
- ‚úÖ Visualization v√† analysis tools

### üé≠ 6 C·∫£m x√∫c ƒë∆∞·ª£c nh·∫≠n di·ªán

| Code | T√™n ti·∫øng Anh | T√™n ti·∫øng Vi·ªát | Emoji |
|------|---------------|----------------|-------|
| ANG  | Anger         | Gi·∫≠n d·ªØ        | üò†    |
| HAP  | Happiness     | Vui v·∫ª         | üòä    |
| SAD  | Sadness       | Bu·ªìn b√£        | üò¢    |
| NEU  | Neutral       | Trung t√≠nh     | üòê    |
| DIS  | Disgust       | Gh√™ t·ªüm        | ü§¢    |
| FEA  | Fear          | S·ª£ h√£i         | üò®    |

---

## üìä Dataset

### CREMA-D (Crowd-sourced Emotional Multimodal Actors Dataset)

- **T·ªïng s·ªë files**: 7,442 audio clips
- **Format**: WAV files (16-bit, mono)
- **Ngu·ªìn**: 91 di·ªÖn vi√™n (48 nam, 43 n·ªØ)
- **ƒê·ªô d√†i**: Kho·∫£ng 3 seconds m·ªói file
- **Sample rate**: 16kHz (ƒë∆∞·ª£c resample l√™n 22.05kHz)

#### Ph√¢n b·ªë c·∫£m x√∫c

| Emotion | S·ªë l∆∞·ª£ng | T·ª∑ l·ªá |
|---------|----------|-------|
| ANG     | 1,271    | 17.08% |
| HAP     | 1,271    | 17.08% |
| SAD     | 1,271    | 17.08% |
| NEU     | 1,087    | 14.61% |
| DIS     | 1,271    | 17.08% |
| FEA     | 1,271    | 17.08% |

#### Format t√™n file

```
{ActorID}_{SentenceID}_{Emotion}_{EmotionLevel}.wav
```

V√≠ d·ª•: `1001_DFA_ANG_XX.wav`
- `1001`: Actor ID
- `DFA`: Sentence ID
- `ANG`: Emotion (Anger)
- `XX`: Emotion level

---

## üèóÔ∏è Ki·∫øn tr√∫c Model

### Audio Processing Pipeline

```
Audio (WAV) 
    ‚Üì
Librosa Load (22.05kHz, mono)
    ‚Üì
Mel Spectrogram (128 bands, log scale)
    ‚Üì
Pad/Crop to 128√ó128
    ‚Üì
Normalize (Z-score)
    ‚Üì
CNN Input (128, 128, 1)
```

### CNN Architecture

```python
Input: (128, 128, 1)
    ‚Üì
Conv2D(32) + BatchNorm + MaxPool + Dropout(0.25)
    ‚Üì
Conv2D(64) + BatchNorm + MaxPool + Dropout(0.25)
    ‚Üì
Conv2D(128) + BatchNorm + MaxPool + Dropout(0.25)
    ‚Üì
Conv2D(256) + BatchNorm + MaxPool + Dropout(0.25)
    ‚Üì
Flatten
    ‚Üì
Dense(512) + BatchNorm + Dropout(0.5)
    ‚Üì
Dense(256) + Dropout(0.5)
    ‚Üì
Dense(6, softmax)
```

**Parameters**:
- Total params: ~2-3M
- Trainable params: ~2-3M
- Optimizer: Adam (lr=0.001)
- Loss: Sparse Categorical Crossentropy

---

## üöÄ C√†i ƒë·∫∑t

### Y√™u c·∫ßu h·ªá th·ªëng

- Python 3.9+
- Conda (recommended)
- 8GB RAM minimum
- GPU (optional, but recommended)

### B∆∞·ªõc 1: Clone repository

```bash
cd voice
```

### B∆∞·ªõc 2: T·∫°o m√¥i tr∆∞·ªùng

```bash
# S·ª≠ d·ª•ng Conda (recommended)
conda create -n voice-recognition python=3.9 -y
conda activate voice-recognition

# Ho·∫∑c s·ª≠ d·ª•ng venv
python -m venv venv
source venv/bin/activate  # Linux/Mac
# ho·∫∑c
venv\Scripts\activate  # Windows
```

### B∆∞·ªõc 3: C√†i ƒë·∫∑t dependencies

```bash
pip install -r requirements.txt
```

### B∆∞·ªõc 4: Verify installation

```bash
python src/config.py
```

K·∫øt qu·∫£ mong ƒë·ª£i:
```
======================================================================
SPEECH EMOTION RECOGNITION - CONFIGURATION
======================================================================
...
‚úì Found 7442 WAV files in audio directory
```

---

## üíª S·ª≠ d·ª•ng

### üéØ Quick Start

```bash
# Ch·∫°y to√†n b·ªô pipeline (data prep + training + evaluation)
python run_pipeline.py

# Ho·∫∑c ch·∫°y nhanh (10 epochs ƒë·ªÉ test)
python run_pipeline.py --quick
```

### 1Ô∏è‚É£ Training Model

#### Option A: S·ª≠ d·ª•ng pipeline runner

```bash
python run_pipeline.py --epochs 50 --batch-size 32
```

#### Option B: Training tr·ª±c ti·∫øp

```bash
python src/train.py
```

#### Option C: Training trong Python

```python
from src.train import train_model

model, history = train_model(
    data_dir='CREMA-D/AudioWAV',
    batch_size=32,
    epochs=50,
    learning_rate=0.001
)
```

### 2Ô∏è‚É£ Evaluation

```bash
python src/evaluate.py
```

Ho·∫∑c:

```python
from src.evaluate import evaluate_model

metrics = evaluate_model(
    model_path='models/final/emotion_model.keras',
    X_test=X_test,
    y_test=y_test
)
```

### 3Ô∏è‚É£ Prediction t·ª´ file

#### CLI

```bash
python src/predict.py
```

#### Python API

```python
from src.predict import predict_from_file

# Predict single file
result = predict_from_file('path/to/audio.wav')

print(f"Emotion: {result['predicted_emotion']}")
print(f"Confidence: {result['confidence']:.2%}")
print(f"Top 3: {result['top_k_predictions']}")
```

#### Batch Prediction

```python
from src.predict import EmotionPredictor

predictor = EmotionPredictor('models/final/emotion_model.keras')

# Predict multiple files
audio_files = ['audio1.wav', 'audio2.wav', 'audio3.wav']
results = predictor.predict_batch(audio_files)

for result in results:
    print(f"{result['file']}: {result['predicted_emotion']} ({result['confidence']:.2%})")
```

### 4Ô∏è‚É£ Recording v√† Real-time Prediction

#### CLI

```bash
python src/record.py
```

#### Python API

```python
from src.record import record_and_predict

# Record 3 seconds v√† predict
result = record_and_predict(
    duration=3.0,
    save=True,
    play=False
)

print(f"Detected emotion: {result['predicted_emotion']}")
```

#### Continuous Recording

```python
from src.record import RealTimeEmotionRecognizer

recognizer = RealTimeEmotionRecognizer()

# Record 5 times, 3 seconds each
results = recognizer.continuous_recognition(
    duration=3.0,
    num_recordings=5,
    delay=1.0
)
```

### 5Ô∏è‚É£ Web Demo v·ªõi Streamlit

```bash
streamlit run demo/app.py
```

M·ªü browser t·∫°i: `http://localhost:8501`

Features:
- üìÅ Upload audio files
- üéôÔ∏è Record from microphone
- üìä Batch processing
- üìà Probability visualization
- üíæ Download predictions

### 6Ô∏è‚É£ Exploratory Data Analysis

```bash
jupyter notebook notebooks/exploratory.ipynb
```

---

## üìÅ C·∫•u tr√∫c Project

```
voice/
‚îÇ
‚îú‚îÄ‚îÄ CREMA-D/                        # Dataset directory
‚îÇ   ‚îú‚îÄ‚îÄ AudioWAV/                   # 7,442 WAV files
‚îÇ   ‚îú‚îÄ‚îÄ AudioMP3/                   # MP3 versions
‚îÇ   ‚îú‚îÄ‚îÄ metadata.csv                # Generated metadata
‚îÇ   ‚îî‚îÄ‚îÄ cache/                      # Preprocessed data cache
‚îÇ
‚îú‚îÄ‚îÄ src/                            # Source code
‚îÇ   ‚îú‚îÄ‚îÄ __init__.py                 # Package init
‚îÇ   ‚îú‚îÄ‚îÄ config.py                   # üîß Configuration
‚îÇ   ‚îú‚îÄ‚îÄ preprocess.py               # üéµ Audio preprocessing
‚îÇ   ‚îú‚îÄ‚îÄ preprocess.ipynb            # üìì Notebook version
‚îÇ   ‚îú‚îÄ‚îÄ dataset.py                  # üìä Dataset management
‚îÇ   ‚îú‚îÄ‚îÄ model.py                    # üß† CNN model
‚îÇ   ‚îú‚îÄ‚îÄ train.py                    # üéì Training pipeline
‚îÇ   ‚îú‚îÄ‚îÄ evaluate.py                 # üìà Evaluation
‚îÇ   ‚îú‚îÄ‚îÄ predict.py                  # üîÆ Inference
‚îÇ   ‚îî‚îÄ‚îÄ record.py                   # üé§ Recording
‚îÇ
‚îú‚îÄ‚îÄ demo/                           # Demo applications
‚îÇ   ‚îî‚îÄ‚îÄ app.py                      # üåê Streamlit web app
‚îÇ
‚îú‚îÄ‚îÄ notebooks/                      # Jupyter notebooks
‚îÇ   ‚îî‚îÄ‚îÄ exploratory.ipynb           # üìä EDA notebook
‚îÇ
‚îú‚îÄ‚îÄ models/                         # Saved models
‚îÇ   ‚îú‚îÄ‚îÄ checkpoints/                # Training checkpoints
‚îÇ   ‚îî‚îÄ‚îÄ final/                      # Final trained models
‚îÇ
‚îú‚îÄ‚îÄ results/                        # Training results
‚îÇ   ‚îú‚îÄ‚îÄ logs/                       # Training logs, TensorBoard
‚îÇ   ‚îú‚îÄ‚îÄ metrics/                    # Evaluation metrics
‚îÇ   ‚îú‚îÄ‚îÄ plots/                      # Visualizations
‚îÇ   ‚îî‚îÄ‚îÄ recordings/                 # Saved recordings
‚îÇ
‚îú‚îÄ‚îÄ requirements.txt                # Dependencies
‚îú‚îÄ‚îÄ run_pipeline.py                 # Pipeline runner
‚îú‚îÄ‚îÄ README.md                       # This file
‚îú‚îÄ‚îÄ QUICKSTART.md                   # Quick start guide
‚îú‚îÄ‚îÄ PROJECT_SUMMARY.md              # Project summary
‚îî‚îÄ‚îÄ .gitignore                      # Git ignore rules
```

---

## üìö API Documentation

### Module: `config.py`

Central configuration file.

**Key Constants:**
```python
EMOTION_MAP: Dict[str, int]        # Emotion to index mapping
EMOTION_NAMES: List[str]           # List of emotions
SAMPLE_RATE: int = 22050           # Audio sample rate
N_MELS: int = 128                  # Number of Mel bands
SPECTROGRAM_SHAPE: Tuple = (128, 128)  # Fixed shape
BATCH_SIZE: int = 32               # Training batch size
EPOCHS: int = 100                  # Training epochs
```

**Functions:**
```python
get_model_path(name, timestamp) -> Path
get_checkpoint_path(timestamp) -> Path
print_config() -> None
```

### Module: `preprocess.py`

Audio preprocessing utilities.

**Functions:**
```python
load_audio(file_path, sr) -> np.ndarray
audio_to_mel_spectrogram(audio, sr, n_mels, n_fft, hop_length) -> np.ndarray
pad_or_crop_spectrogram(spectrogram, target_shape) -> np.ndarray
process_audio_file(file_path) -> Tuple[np.ndarray, int]
load_dataset(data_dir) -> Tuple[np.ndarray, np.ndarray]
```

### Module: `dataset.py`

Dataset management and metadata handling.

**Class: `CremaDDataset`**
```python
parse_filename(filename) -> Dict
create_metadata_csv(output_path) -> pd.DataFrame
get_emotion_distribution() -> pd.DataFrame
filter_by_emotion(emotions) -> pd.DataFrame
save_processed_data(X, y, output_dir, prefix) -> None
load_processed_data(data_dir, prefix) -> Tuple
```

### Module: `model.py`

CNN model architecture.

**Functions:**
```python
create_cnn_model(input_shape, num_classes) -> keras.Model
compile_model(model, learning_rate) -> keras.Model
create_model(input_shape, num_classes, learning_rate) -> keras.Model
```

### Module: `train.py`

Training pipeline.

**Function:**
```python
train_model(
    data_dir: str,
    batch_size: int = 32,
    epochs: int = 50,
    validation_split: float = 0.2,
    test_split: float = 0.1,
    learning_rate: float = 0.001
) -> Tuple[keras.Model, keras.callbacks.History]
```

### Module: `evaluate.py`

Model evaluation and metrics.

**Class: `ModelEvaluator`**
```python
evaluate(X, y) -> Dict
plot_confusion_matrix(normalize, save_path) -> None
plot_roc_curves(save_path) -> None
generate_classification_report(save_path) -> str
full_evaluation(X_test, y_test, save_results) -> Dict
```

### Module: `predict.py`

Inference and prediction.

**Class: `EmotionPredictor`**
```python
predict(audio_path, return_probabilities) -> Dict
predict_batch(audio_paths, verbose) -> List[Dict]
print_prediction(result) -> None
```

**Convenience Function:**
```python
predict_from_file(audio_path, model_path, verbose) -> Dict
```

### Module: `record.py`

Audio recording and real-time recognition.

**Class: `AudioRecorder`**
```python
record(duration, device) -> np.ndarray
save(output_path, audio) -> Path
play(audio) -> None
record_and_save(duration, output_dir) -> Path
```

**Class: `RealTimeEmotionRecognizer`**
```python
recognize(duration, save_recording, play_back) -> Dict
continuous_recognition(duration, num_recordings, delay) -> List[Dict]
```

---

## üìä K·∫øt qu·∫£

### Training Performance

| Metric | Value |
|--------|-------|
| Test Accuracy | 60-70% |
| Training Time | 30-60 min (50 epochs, GPU) |
| Model Size | 50-100 MB |
| Inference Time | < 1 second/file |

### Per-Class Performance

| Emotion | Precision | Recall | F1-Score |
|---------|-----------|--------|----------|
| ANG     | 0.65-0.75 | 0.60-0.70 | 0.62-0.72 |
| HAP     | 0.60-0.70 | 0.55-0.65 | 0.57-0.67 |
| SAD     | 0.70-0.80 | 0.65-0.75 | 0.67-0.77 |
| NEU     | 0.55-0.65 | 0.50-0.60 | 0.52-0.62 |
| DIS     | 0.60-0.70 | 0.55-0.65 | 0.57-0.67 |
| FEA     | 0.65-0.75 | 0.60-0.70 | 0.62-0.72 |

### Training Curves

Training v√† validation accuracy th∆∞·ªùng converge sau 30-40 epochs.

---

## üé¨ Demo

### Screenshots

#### 1. Streamlit Web App
- Upload audio files
- Real-time prediction
- Probability visualization

#### 2. CLI Prediction
```bash
$ python src/predict.py

Loading model from models/final/emotion_model.keras...
‚úì Model loaded successfully

============================================================
FILE: sample_audio.wav
============================================================
‚úì Predicted Emotion: ANG (Anger)
   Confidence: 87.35%

Top 3 Predictions:
  1. ANG (Anger): 87.35%
  2. DIS (Disgust): 8.12%
  3. FEA (Fear): 2.43%
============================================================
```

#### 3. Real-time Recording
```bash
$ python src/record.py

üé§ Recording for 3.0 seconds...
Speak now!
‚úì Recording complete!
‚úì Audio saved to results/recordings/recording_20260105_123456.wav

üß† Analyzing emotion...
============================================================
Predicted Emotion: HAP (Happiness)
Confidence: 72.18%
============================================================
```

---

## üîß Configuration

### T√πy ch·ªânh Hyperparameters

Edit `src/config.py`:

```python
# Training parameters
BATCH_SIZE = 32
EPOCHS = 100
LEARNING_RATE = 0.001

# Model architecture
CNN_PARAMS = {
    'conv_blocks': [
        {'filters': 32, 'kernel_size': (3, 3), 'pool_size': (2, 2)},
        {'filters': 64, 'kernel_size': (3, 3), 'pool_size': (2, 2)},
        # Add more layers...
    ],
    'dense_layers': [512, 256],
    'dropout_rate': 0.5
}

# Audio processing
SAMPLE_RATE = 22050
N_MELS = 128
N_FFT = 2048
HOP_LENGTH = 512
```

### Data Augmentation

```python
AUGMENTATION_PARAMS = {
    'time_stretch': {'enabled': True, 'rate_range': (0.8, 1.2)},
    'pitch_shift': {'enabled': True, 'n_steps_range': (-2, 2)},
    'noise_injection': {'enabled': True, 'noise_factor': 0.005},
    'time_shift': {'enabled': True, 'shift_max': 0.2}
}
```

---

## üêõ Troubleshooting

### L·ªói: No module named 'librosa'

```bash
pip install librosa
```

### L·ªói: Numba caching

```bash
export NUMBA_CACHE_DIR=/tmp
python src/train.py
```

### L·ªói: Out of memory

Gi·∫£m batch size trong `config.py`:
```python
BATCH_SIZE = 16  # Default: 32
```

### L·ªói: GPU not available

TensorFlow s·∫Ω t·ª± ƒë·ªông s·ª≠ d·ª•ng CPU. ƒê·ªÉ verify:
```python
import tensorflow as tf
print("GPU Available:", tf.config.list_physical_devices('GPU'))
```

### L·ªói: Recording kh√¥ng ho·∫°t ƒë·ªông

```bash
pip install sounddevice soundfile

# Ki·ªÉm tra audio devices
python -c "import sounddevice; print(sounddevice.query_devices())"
```

---

## üìà Roadmap

### Version 1.0 (Current)
- ‚úÖ Basic CNN model
- ‚úÖ 6 emotion classes
- ‚úÖ Mel spectrogram features
- ‚úÖ Web interface
- ‚úÖ Real-time recording

### Version 1.1 (Planned)
- ‚è≥ Data augmentation improvements
- ‚è≥ Ensemble models
- ‚è≥ Transfer learning (VGGish, YAMNet)
- ‚è≥ Multi-language support

### Version 2.0 (Future)
- ‚è≥ LSTM/GRU for temporal features
- ‚è≥ Attention mechanisms
- ‚è≥ Multi-modal (audio + text)
- ‚è≥ Mobile deployment
- ‚è≥ REST API

---

## ü§ù Contributing

Contributions are welcome! Please feel free to submit a Pull Request.

1. Fork the project
2. Create your feature branch (`git checkout -b feature/AmazingFeature`)
3. Commit your changes (`git commit -m 'Add some AmazingFeature'`)
4. Push to the branch (`git push origin feature/AmazingFeature`)
5. Open a Pull Request

---

## üìù License

This project is licensed under the MIT License - see the [LICENSE](LICENSE) file for details.

---

## üìñ Citation

N·∫øu s·ª≠ d·ª•ng project n√†y, vui l√≤ng cite dataset CREMA-D:

```bibtex
@article{cao2014crema,
  title={CREMA-D: Crowd-sourced Emotional Multimodal Actors Dataset},
  author={Cao, Houwei and Cooper, David G and Keutmann, Michael K and Gur, Ruben C and Nenkova, Ani and Verma, Ragini},
  journal={IEEE Transactions on Affective Computing},
  volume={5},
  number={4},
  pages={377--390},
  year={2014},
  publisher={IEEE}
}
```

---

## üë• Authors

- **Your Name** - Initial work

---

## üôè Acknowledgments

- CREMA-D dataset creators
- TensorFlow and Keras teams
- Librosa library developers
- Open source community

---

## üìû Contact

- GitHub: [@yourusername](https://github.com/yourusername)
- Email: your.email@example.com

---

<div align="center">

**Made with ‚ù§Ô∏è and Python**

‚≠ê Star this repo if you find it helpful!

</div>
