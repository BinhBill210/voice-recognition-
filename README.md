# ğŸ¤ Speech Emotion Recognition

Dá»± Ã¡n nháº­n diá»‡n cáº£m xÃºc tá»« giá»ng nÃ³i sá»­ dá»¥ng Deep Learning vá»›i dataset CREMA-D.

[![Python](https://img.shields.io/badge/Python-3.9+-blue.svg)](https://www.python.org/)
[![TensorFlow](https://img.shields.io/badge/TensorFlow-2.15+-orange.svg)](https://www.tensorflow.org/)
[![License](https://img.shields.io/badge/License-MIT-green.svg)](LICENSE)

---

## ğŸš€ QUICK START - Cháº¡y ngay (3 bÆ°á»›c)

```bash
# 1. Clone repository
git clone https://github.com/BinhBill210/voice-recognition-.git
cd voice-recognition-

# 2. Táº¡o environment vÃ  cÃ i Ä‘áº·t
conda env create -f environment.yml
conda activate voice-recognition

# 3. Cháº¡y test vÃ  training
python test_imports.py
python safe_run.py --quick  # 10 epochs, ~10-15 phÃºt
```

**Hoáº·c vá»›i pip:**
```bash
pip install -r requirements.txt
python run_pipeline.py --quick
```

### âš¡ Commands cÆ¡ báº£n

| Task | Command |
|------|---------|
| **Test imports** | `python test_imports.py` |
| **Quick training** | `python safe_run.py --quick` |
| **Full training** | `python safe_run.py --epochs 50` |
| **Prediction** | `python src/predict.py audio.wav` |
| **Web demo** | `streamlit run demo/app.py` |
| **Recording** | `python src/record.py` |

---

## ğŸ“‹ Má»¥c lá»¥c

- [Giá»›i thiá»‡u](#-giá»›i-thiá»‡u)
- [Dataset](#-dataset)
- [Kiáº¿n trÃºc Model](#-kiáº¿n-trÃºc-model)
- [CÃ i Ä‘áº·t](#-cÃ i-Ä‘áº·t)
- [Sá»­ dá»¥ng](#-sá»­-dá»¥ng)
- [Cáº¥u trÃºc Project](#-cáº¥u-trÃºc-project)
- [API Documentation](#-api-documentation)
- [Troubleshooting](#-troubleshooting)
- [Roadmap](#-roadmap)

---

## ğŸ¯ Giá»›i thiá»‡u

Project nÃ y xÃ¢y dá»±ng má»™t há»‡ thá»‘ng **nháº­n diá»‡n cáº£m xÃºc tá»« giá»ng nÃ³i** sá»­ dá»¥ng Convolutional Neural Network (CNN) vá»›i Mel Spectrogram. Há»‡ thá»‘ng cÃ³ thá»ƒ:

- âœ… Nháº­n diá»‡n 6 cáº£m xÃºc: Anger, Happiness, Sadness, Neutral, Disgust, Fear
- âœ… Xá»­ lÃ½ audio files (WAV format)
- âœ… Ghi Ã¢m real-time tá»« microphone
- âœ… Web interface vá»›i Streamlit
- âœ… Batch prediction
- âœ… Visualization vÃ  analysis tools

### ğŸ­ 6 Cáº£m xÃºc Ä‘Æ°á»£c nháº­n diá»‡n

| Code | TÃªn tiáº¿ng Anh | TÃªn tiáº¿ng Viá»‡t | Emoji |
|------|---------------|----------------|-------|
| ANG  | Anger         | Giáº­n dá»¯        | ğŸ˜     |
| HAP  | Happiness     | Vui váº»         | ğŸ˜Š    |
| SAD  | Sadness       | Buá»“n bÃ£        | ğŸ˜¢    |
| NEU  | Neutral       | Trung tÃ­nh     | ğŸ˜    |
| DIS  | Disgust       | GhÃª tá»Ÿm        | ğŸ¤¢    |
| FEA  | Fear          | Sá»£ hÃ£i         | ğŸ˜¨    |

---

## ğŸ“Š Dataset

### CREMA-D (Crowd-sourced Emotional Multimodal Actors Dataset)

- **ğŸ“¦ Files:** 7,442 audio clips
- **ğŸ­ Emotions:** 6 classes (ANG, HAP, SAD, NEU, DIS, FEA)
- **ğŸ‘¥ Speakers:** 91 actors (48 male, 43 female)
- **ğŸšï¸ Format:** WAV, 16kHz
- **â±ï¸ Duration:** ~2-3 seconds per clip

**TÃªn file format:** `ActorID_Sentence_Emotion_Intensity.wav`

VÃ­ dá»¥: `1001_DFA_ANG_XX.wav`
- `1001`: Actor ID
- `DFA`: Sentence identifier  
- `ANG`: Emotion (Anger)
- `XX`: Intensity level

---

## ğŸ—ï¸ Kiáº¿n trÃºc Model

### Pipeline Overview

```
Audio Input (.wav)
    â†“
Preprocessing (librosa)
    â†“
Mel Spectrogram (128 bands)
    â†“
2D CNN (4 Conv blocks)
    â†“
Dense Layers
    â†“
Softmax (6 classes)
    â†“
Emotion Prediction
```

### CNN Architecture

```python
Input: (128, 216, 1)  # Mel spectrogram

Conv Block 1: 64 filters
    â”œâ”€â”€ Conv2D(3x3) + BatchNorm + ReLU
    â”œâ”€â”€ Conv2D(3x3) + BatchNorm + ReLU
    â”œâ”€â”€ MaxPool(2x2)
    â””â”€â”€ Dropout(0.3)

Conv Block 2: 128 filters
Conv Block 3: 256 filters
Conv Block 4: 512 filters

Flatten
    â†“
Dense(512) + BatchNorm + ReLU + Dropout(0.5)
    â†“
Dense(256) + BatchNorm + ReLU + Dropout(0.5)
    â†“
Dense(6) + Softmax

Total Parameters: ~5M
```

---

## ğŸ”§ CÃ i Ä‘áº·t

### Requirements

- **Python:** 3.9+
- **OS:** macOS 10.15+, Ubuntu 18.04+, Windows 10+
- **RAM:** 4GB minimum (8GB recommended)
- **Disk:** 10GB free space

### Option 1: Conda (Khuyáº¿n nghá»‹)

```bash
# Clone repository
git clone https://github.com/BinhBill210/voice-recognition-.git
cd voice-recognition-

# Táº¡o environment tá»« file
conda env create -f environment.yml
conda activate voice-recognition

# Verify
python test_imports.py
```

### Option 2: Pip

```bash
# Clone repository
git clone https://github.com/BinhBill210/voice-recognition-.git
cd voice-recognition-

# Táº¡o virtual environment
python3.9 -m venv venv
source venv/bin/activate  # macOS/Linux
# hoáº·c: venv\Scripts\activate  # Windows

# Install
pip install -r requirements.txt

# Verify
python test_imports.py
```

---

## ğŸ’» Sá»­ dá»¥ng

### 1. Training

```bash
# Quick test (10 epochs, ~10-15 phÃºt)
python safe_run.py --quick

# Full training (50 epochs, ~45-60 phÃºt)
python safe_run.py --epochs 50

# Custom
python run_pipeline.py --epochs 30 --batch-size 16
```

**Output:**
```
Train set: 5,358 samples
Validation set: 1,340 samples
Test set: 744 samples

Test Accuracy: 0.7250 (72.50%)

Per-class accuracy:
  ANG: 0.7450 (74.50%)
  HAP: 0.6890 (68.90%)
  SAD: 0.7320 (73.20%)
  NEU: 0.7150 (71.50%)
  DIS: 0.7080 (70.80%)
  FEA: 0.7610 (76.10%)
```

### 2. Prediction

```bash
# Single file
python src/predict.py data/CREMA-D/AudioWAV/1001_DFA_ANG_XX.wav

# Output:
# Emotion: ANG (Anger)
# Confidence: 0.89
# Probabilities:
#   ANG: 89.2%
#   DIS: 5.3%
#   NEU: 2.1%
#   ...
```

### 3. Recording & Real-time Prediction

```bash
python src/record.py

# Output:
# Recording... (Press Ctrl+C to stop)
# Detected emotion: HAP (Happiness)
# Confidence: 0.76
```

### 4. Web Demo

```bash
streamlit run demo/app.py

# Opens browser at http://localhost:8501
# Features:
# - Upload audio file
# - Record from microphone
# - View spectrogram
# - See predictions
```

### 5. Batch Prediction

```python
from src.predict import EmotionPredictor

predictor = EmotionPredictor('best_model.keras')

files = ['audio1.wav', 'audio2.wav', 'audio3.wav']
results = predictor.predict_batch(files)

for file, (emotion, prob) in zip(files, results):
    print(f"{file}: {emotion} ({prob:.2%})")
```

---

## ğŸ“ Cáº¥u trÃºc Project

```
voice-recognition-/
â”‚
â”œâ”€â”€ data/
â”‚   â””â”€â”€ CREMA-D/
â”‚       â””â”€â”€ AudioWAV/           # 7,442 audio files
â”‚
â”œâ”€â”€ src/                         # Source code (9 modules)
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ config.py               # Configuration
â”‚   â”œâ”€â”€ preprocess.py           # Audio preprocessing
â”‚   â”œâ”€â”€ dataset.py              # Dataset management
â”‚   â”œâ”€â”€ model.py                # CNN architecture
â”‚   â”œâ”€â”€ train.py                # Training script
â”‚   â”œâ”€â”€ evaluate.py             # Evaluation
â”‚   â”œâ”€â”€ predict.py              # Prediction
â”‚   â””â”€â”€ record.py               # Audio recording
â”‚
â”œâ”€â”€ demo/
â”‚   â””â”€â”€ app.py                  # Streamlit web app
â”‚
â”œâ”€â”€ notebooks/
â”‚   â””â”€â”€ exploratory.ipynb       # EDA notebook
â”‚
â”œâ”€â”€ models/                      # Saved models
â”‚   â”œâ”€â”€ checkpoints/
â”‚   â””â”€â”€ final/
â”‚
â”œâ”€â”€ results/                     # Training results
â”‚   â”œâ”€â”€ logs/
â”‚   â”œâ”€â”€ metrics/
â”‚   â””â”€â”€ plots/
â”‚
â”œâ”€â”€ run_pipeline.py             # Main pipeline runner
â”œâ”€â”€ safe_run.py                 # macOS-safe wrapper
â”œâ”€â”€ test_imports.py             # Test script
â”‚
â”œâ”€â”€ environment.yml             # Conda environment
â”œâ”€â”€ requirements.txt            # Pip dependencies
â””â”€â”€ README.md                   # This file
```

---

## ğŸ“š API Documentation

### `src/config.py`

Central configuration file.

```python
from src.config import (
    AUDIO_WAV_DIR,       # Path to audio files
    EMOTION_MAP,         # Emotion code â†’ label
    EMOTION_NAMES,       # List of emotion names
    SAMPLE_RATE,         # 22050 Hz
    N_MELS,              # 128 mel bands
    SPECTROGRAM_SHAPE,   # (128, 216)
    NUM_CLASSES,         # 6
)
```

### `src/preprocess.py`

Audio preprocessing functions.

```python
from src.preprocess import (
    extract_emotion_from_filename,  # Parse emotion from filename
    load_audio,                      # Load WAV file
    audio_to_melspectrogram,         # Convert to mel spec
    pad_or_crop_spectrogram,         # Normalize shape
    process_audio_file,              # All-in-one
    load_dataset,                    # Load full dataset
)

# Example
spec, label = process_audio_file('audio.wav')
X, y = load_dataset('data/CREMA-D/AudioWAV')
```

### `src/model.py`

CNN model definition.

```python
from src.model import create_model

model = create_model(
    input_shape=(128, 216, 1),
    num_classes=6,
    learning_rate=0.001
)

model.summary()
```

### `src/train.py`

Training functions.

```python
from src.train import train_model

model, history = train_model(
    data_dir='data/CREMA-D/AudioWAV',
    batch_size=32,
    epochs=50,
    validation_split=0.2,
    test_split=0.1,
    learning_rate=0.001
)
```

### `src/predict.py`

Prediction interface.

```python
from src.predict import EmotionPredictor

predictor = EmotionPredictor('best_model.keras')

# Single prediction
emotion, probs = predictor.predict('audio.wav')

# Batch prediction
results = predictor.predict_batch(['audio1.wav', 'audio2.wav'])
```

### `src/record.py`

Audio recording.

```python
from src.record import record_audio, realtime_predict

# Record audio
audio = record_audio(duration=3, sample_rate=22050)

# Real-time prediction
realtime_predict(predictor, duration=3)
```

---

## âš ï¸ Troubleshooting

### ğŸ macOS: Mutex Lock Warning

```
[mutex.cc : 452] RAW: Lock blocking 0x102b754b8
```

**Giáº£i phÃ¡p:**
```bash
# Option 1: DÃ¹ng safe wrapper (KHUYáº¾N NGHá»Š)
python safe_run.py --quick

# Option 2: Set environment variables
NUMBA_CACHE_DIR=/tmp python run_pipeline.py --quick

# Option 3: Ignore (warning khÃ´ng áº£nh hÆ°á»Ÿng chá»©c nÄƒng)
```

### âŒ Module Not Found

```bash
# Reinstall dependencies
pip install -r requirements.txt

# Or with conda
conda env create -f environment.yml
```

### âŒ AudioWAV Directory Not Found

```bash
# Check path
python -c "import sys; sys.path.append('src'); from config import AUDIO_WAV_DIR; print(AUDIO_WAV_DIR)"

# Should be: /path/to/voice/data/CREMA-D/AudioWAV
```

### âŒ Out of Memory

```bash
# Reduce batch size
python run_pipeline.py --quick --batch-size 16

# Or 8
python safe_run.py --quick --batch-size 8
```

### âŒ Streamlit Demo Crashes

```bash
# Kill all Python processes
pkill -9 python

# Restart terminal
conda activate voice-recognition
streamlit run demo/app.py
```

---

## ğŸ“ˆ Káº¿t quáº£ mong Ä‘á»£i

### Performance

| Metric | Value |
|--------|-------|
| **Test Accuracy** | 70-75% |
| **Training Time** | 10-15 min (10 epochs) |
| **Inference Time** | < 1 second/file |
| **Model Size** | ~20MB |

### Per-Class Performance

| Emotion | Accuracy | Notes |
|---------|----------|-------|
| Anger (ANG) | 73-76% | Tá»‘t nháº¥t |
| Fear (FEA) | 74-77% | Tá»‘t |
| Sadness (SAD) | 71-74% | KhÃ¡ tá»‘t |
| Neutral (NEU) | 70-73% | Trung bÃ¬nh |
| Disgust (DIS) | 69-72% | Trung bÃ¬nh |
| Happiness (HAP) | 67-70% | KhÃ³ nháº¥t |

---

## ğŸ® Demo

### Streamlit Web App

![Demo Screenshot](demo_screenshot.png)

**Features:**
- Upload audio file
- Record from microphone
- View waveform and spectrogram
- See prediction probabilities
- Interactive visualization

```bash
streamlit run demo/app.py
```

### CLI Demo

```bash
# Predict from file
python src/predict.py sample.wav

# Record and predict
python src/record.py
```

---

## ğŸ—ºï¸ Roadmap

- [x] Basic CNN model
- [x] Data preprocessing
- [x] Training pipeline
- [x] Prediction API
- [x] Web demo
- [x] Real-time recording
- [ ] Data augmentation improvements
- [ ] Transfer learning (VGGish, YAMNet)
- [ ] LSTM/Transformer models
- [ ] Multi-language support
- [ ] Mobile deployment (TFLite)
- [ ] REST API (FastAPI)
- [ ] Docker containerization

---

## ğŸ¤ Contributing

Contributions are welcome! Please:

1. Fork the repository
2. Create your feature branch (`git checkout -b feature/AmazingFeature`)
3. Commit your changes (`git commit -m 'Add some AmazingFeature'`)
4. Push to the branch (`git push origin feature/AmazingFeature`)
5. Open a Pull Request

---

## ğŸ“„ License

This project is licensed under the MIT License - see the [LICENSE](LICENSE) file for details.

---

## ğŸ“š References

### Dataset
- CREMA-D: https://github.com/CheyneyComputerScience/CREMA-D
- Paper: Cao et al. (2014). CREMA-D: Crowd-sourced Emotional Multimodal Actors Dataset

### Libraries
- TensorFlow: https://www.tensorflow.org/
- Librosa: https://librosa.org/
- Streamlit: https://streamlit.io/

---

## ğŸ“§ Contact

- **GitHub:** https://github.com/BinhBill210/voice-recognition-.git
- **Issues:** https://github.com/BinhBill210/voice-recognition-/issues

---

## ğŸ™ Acknowledgments

- CREMA-D dataset creators
- TensorFlow team
- Librosa developers
- Open source community

---

**â­ If you find this project useful, please give it a star!**

**Last updated:** January 5, 2026

